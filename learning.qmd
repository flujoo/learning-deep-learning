# Learning

```{python}
import numpy as np
```

The learning of a neural network is the process of finding the weights that produce some minimum value of its loss function.


## Loss Functions

The loss function of a neural network is a negative indicator of its accuracy. It measures the distance between the supervised data and the output of the neural network.

Here we introduce two loss functions.

**Mean squared error**:

$$\displaystyle E =
\sum_{k} (y_k - t_k)^2$$

The meanings of these symbols:

- $k$ is the dimension of the data.
- $y_k$ is the output.
- $t_k$ is the supervised data.

**Cross entropy error**:

$$\displaystyle E =
- \sum_{k} t_k \: \mathrm{log} \, y_k$$

Let's try to understand the cross entropy error. For one-hot data, $t_{some} = 1$ for some dimension, while $t_k = 0$ for other dimensions. Therefore, the formula can be reduced to $E = - \mathrm{log} \, y_{some}$, and the larger $y_{some}$ is, the lower $E$ will be. When $y_{some} = 1$, $E = 0$. This makes perfect sense.


## Gradient Method

Gradient method is the algorithm for finding the weights that produce some minimum value of a neural network's loss function. 

Gradient? Let's first recall the definition of **derivative**:

$$\displaystyle \frac{\mathrm{d}f(x)}{\mathrm{d}x}
= \lim\limits_{h \to 0} \frac{f(x+h) - f(x)}{h}$$

For a multivariate function, we can find its derivative with respect to some variable by treating other variables as constants. The outcome is called **partial derivative**.

Finally, the **gradient** of a function is the vector of its partial derivatives:

$$\nabla f = \left(\frac{\partial f}{\partial x_0},
\frac{\partial f}{\partial x_1}, \dots \right)$$

The process of finding the derivative(s) of a function is called **differentiation**.

Gradient method steps:

1. Suppose you are at a point of $f$.
2. Calculate the gradient of this point.
3. Move a little bit against the gradient.
4. Repeat step 2 and 3 until reach a local minimum.


## Implementation

First, let's implement a function for calculating derivative: 

```{python}
def get_derivative(f, x):
  h = 1e-4
  d = (f(x+h) - f(x-h)) / (2*h)
  return d
```

Try it on $f(x) = x^2$ at $x = 2$. Its derivative is $f'(x) = 2x$, so the outcome should be $4$:

```{python}
get_derivative(lambda x: x**2, 2)
```

Second, let's implement a function for calculating gradient:

```{python}
def get_gradient(f, x):
  e = 1e-4
  g = np.zeros_like(x, float)

  for i, _ in enumerate(x):
    h = np.zeros_like(x, float)
    h[i] = e
    g[i] = (f(x+h) - f(x-h)) / (2*e)

  return g
```

Try it on $f(x_0, x_1) = x_0^2 + x_1^2$ at $x_0 = 2$ and $x_1 = 3$. Its gradient is $(2x_0, 2x_1)$, so the outcome should be $(4, 6)$:

```{python}
get_gradient(lambda x: x[0]**2 + x[1]**2, np.array([2, 3]))
```

Finally, let's implement the gradient method:

```{python}
def gradient_method(f, x, rate=0.1, step=100):
  for i in range(step):
    g = get_gradient(f, x)
    x = x - g*rate

  return x
```

`rate` is **learning rate**.

Find the $(x_0, x_1)$ where $f(x_0, x_1) = x_0^2 + x_1^2$ has its minimum:

```{python}
gradient_method(lambda x: x[0]**2 + x[1]**2, np.array([-3, 4]))
```
