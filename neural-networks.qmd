---
title: "Neural Networks"

format: 
  html:
    fig-width: 4
    fig-height: 3
---

```{python}
import numpy as np
import matplotlib.pylab as plt
```


## Basic Structure

Below is a very simple neural network:

![](assets/neural-networks/perceptron.png)

The meanings of these symbols:

- $x_1$ and $x_2$ are the input.
- $w_1$ and $w_2$ are weights.
- $y$ is the output.

The computation in the neural network:

- $a = x_1 w_1 + x_2 w_2 + b$
- $y = h(a)$

Explanations:

- $a$ is the weighted sum of the input.
- $b$ is bias.
- $h(x)$ is activation function.


## Activation Functions

Here we introduce three common activation functions.

**Sigmoid function**:

$$\displaystyle h(x) = \frac{1}{1 + \mathrm{exp}(-x)}$$

Implement it:

```{python}
def sigmoid(x):
  y = 1 / (1 + np.exp(-x))
  return y
```

Plot it:

```{python}
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.show()
```

**Rectified linear unit** (ReLU):

$$h(x) =
\begin{cases}
  x & (x > 0) \\
  0 & (x \le 0)
\end{cases}$$

Implement it:

```{python}
def relu(x):
  y = np.maximum(0, x)
  return y
```

Plot it:

```{python}
x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)
plt.plot(x, y)
plt.show()
```

**Softmax function** is used in output layers and is for multi-class classification problems:

$$y_k = \frac{\mathrm{exp}(a_k)}{
\displaystyle\sum^{n}_{i = 1} \mathrm{exp}(a_i)}$$
