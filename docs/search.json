[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Deep Learning",
    "section": "",
    "text": "Turning my learning of deep learning into a textbook."
  },
  {
    "objectID": "neural-networks.html",
    "href": "neural-networks.html",
    "title": "1  Neural Networks",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pylab as plt"
  },
  {
    "objectID": "neural-networks.html#basic-structure",
    "href": "neural-networks.html#basic-structure",
    "title": "1  Neural Networks",
    "section": "1.1 Basic Structure",
    "text": "1.1 Basic Structure\nBelow is a very simple neural network:\n\nThe meanings of these symbols:\n\n\\(x_1\\) and \\(x_2\\) are the input.\n\\(w_1\\) and \\(w_2\\) are weights.\n\\(y\\) is the output.\n\nThe computation in the neural network:\n\n\\(a = x_1 w_1 + x_2 w_2 + b\\)\n\\(y = h(a)\\)\n\nExplanations:\n\n\\(a\\) is the weighted sum of the input.\n\\(b\\) is bias.\n\\(h(x)\\) is activation function."
  },
  {
    "objectID": "neural-networks.html#activation-functions",
    "href": "neural-networks.html#activation-functions",
    "title": "1  Neural Networks",
    "section": "1.2 Activation Functions",
    "text": "1.2 Activation Functions\nHere we introduce three common activation functions.\nSigmoid function:\n\\[\\displaystyle h(x) = \\frac{1}{1 + \\mathrm{exp}(-x)}\\]\nImplement it:\n\ndef sigmoid(x):\n  y = 1 / (1 + np.exp(-x))\n  return y\n\nPlot it:\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = sigmoid(x)\nplt.plot(x, y)\nplt.show()\n\n\n\n\nRectified linear unit (ReLU):\n\\[h(x) =\n\\begin{cases}\n  x & (x > 0) \\\\\n  0 & (x \\le 0)\n\\end{cases}\\]\nImplement it:\n\ndef relu(x):\n  y = np.maximum(0, x)\n  return y\n\nPlot it:\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = relu(x)\nplt.plot(x, y)\nplt.show()\n\n\n\n\nSoftmax function is used in output layers and is for multi-class classification problems:\n\\[y_k = \\frac{\\mathrm{exp}(a_k)}{\n\\displaystyle\\sum^{n}_{i = 1} \\mathrm{exp}(a_i)}\\]"
  },
  {
    "objectID": "learning.html",
    "href": "learning.html",
    "title": "2  Learning",
    "section": "",
    "text": "import numpy as np\nThe learning of a neural network is the process of finding the weights that produce some minimum value of its loss function."
  },
  {
    "objectID": "learning.html#loss-functions",
    "href": "learning.html#loss-functions",
    "title": "2  Learning",
    "section": "2.1 Loss Functions",
    "text": "2.1 Loss Functions\nThe loss function of a neural network is a negative indicator of its accuracy. It measures the distance between the supervised data and the output of the neural network.\nHere we introduce two loss functions.\nMean squared error:\n\\[\\displaystyle E =\n\\sum_{k} (y_k - t_k)^2\\]\nThe meanings of these symbols:\n\n\\(k\\) is the dimension of the data.\n\\(y_k\\) is the output.\n\\(t_k\\) is the supervised data.\n\nCross entropy error:\n\\[\\displaystyle E =\n- \\sum_{k} t_k \\: \\mathrm{log} \\, y_k\\]\nLet’s try to understand the cross entropy error. For one-hot data, \\(t_{some} = 1\\) for some dimension, while \\(t_k = 0\\) for other dimensions. Therefore, the formula can be reduced to \\(E = - \\mathrm{log} \\, y_{some}\\), and the larger \\(y_{some}\\) is, the lower \\(E\\) will be. When \\(y_{some} = 1\\), \\(E = 0\\). This makes perfect sense."
  },
  {
    "objectID": "learning.html#gradient-method",
    "href": "learning.html#gradient-method",
    "title": "2  Learning",
    "section": "2.2 Gradient Method",
    "text": "2.2 Gradient Method\nGradient method is the algorithm for finding the weights that produce some minimum value of a neural network’s loss function.\nGradient? Let’s first recall the definition of derivative:\n\\[\\displaystyle \\frac{\\mathrm{d}f(x)}{\\mathrm{d}x}\n= \\lim\\limits_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\]\nFor a multivariate function, we can find its derivative with respect to some variable by treating other variables as constants. The outcome is called partial derivative.\nFinally, the gradient of a function is the vector of its partial derivatives:\n\\[\\nabla f = \\left(\\frac{\\partial f}{\\partial x_0},\n\\frac{\\partial f}{\\partial x_1}, \\dots \\right)\\]\nThe process of finding the derivative(s) of a function is called differentiation.\nGradient method steps:\n\nSuppose you are at a point of \\(f\\).\nCalculate the gradient of this point.\nMove a little bit against the gradient.\nRepeat step 2 and 3 until reach a local minimum."
  },
  {
    "objectID": "learning.html#implementation",
    "href": "learning.html#implementation",
    "title": "2  Learning",
    "section": "2.3 Implementation",
    "text": "2.3 Implementation\nFirst, let’s implement a function for calculating derivative:\n\ndef get_derivative(f, x):\n  h = 1e-4\n  d = (f(x+h) - f(x-h)) / (2*h)\n  return d\n\nTry it on \\(f(x) = x^2\\) at \\(x = 2\\). Its derivative is \\(f'(x) = 2x\\), so the outcome should be \\(4\\):\n\nget_derivative(lambda x: x**2, 2)\n\n4.000000000004\n\n\nSecond, let’s implement a function for calculating gradient:\n\ndef get_gradient(f, x):\n  e = 1e-4\n  g = np.zeros_like(x, float)\n\n  for i, _ in enumerate(x):\n    h = np.zeros_like(x, float)\n    h[i] = e\n    g[i] = (f(x+h) - f(x-h)) / (2*e)\n\n  return g\n\nTry it on \\(f(x_0, x_1) = x_0^2 + x_1^2\\) at \\(x_0 = 2\\) and \\(x_1 = 3\\). Its gradient is \\((2x_0, 2x_1)\\), so the outcome should be \\((4, 6)\\):\n\nget_gradient(lambda x: x[0]**2 + x[1]**2, np.array([2, 3]))\n\narray([4., 6.])\n\n\nFinally, let’s implement the gradient method:\n\ndef gradient_method(f, x, rate=0.1, step=100):\n  for i in range(step):\n    g = get_gradient(f, x)\n    x = x - g*rate\n\n  return x\n\nrate is learning rate.\nFind the \\((x_0, x_1)\\) where \\(f(x_0, x_1) = x_0^2 + x_1^2\\) has its minimum:\n\ngradient_method(lambda x: x[0]**2 + x[1]**2, np.array([-3, 4]))\n\narray([-6.11110793e-10,  8.14814391e-10])"
  },
  {
    "objectID": "mnist.html",
    "href": "mnist.html",
    "title": "3  MNIST",
    "section": "",
    "text": "from tensorflow.keras.datasets import mnist\nfrom matplotlib import pyplot\nimport numpy as np\nThe MNIST dataset of handwritten digits is widely used. Let’s get familiar with it."
  },
  {
    "objectID": "mnist.html#mnist",
    "href": "mnist.html#mnist",
    "title": "3  MNIST",
    "section": "3.1 MNIST",
    "text": "3.1 MNIST\nThe components of the dataset:\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\nThe shapes of the components:\n\ntrain_images.shape\n\n(60000, 28, 28)\n\n\n\ntrain_labels.shape\n\n(60000,)\n\n\nEach case of the images is a 28 × 28 array which represents a digit. Each number in the array is between 0 and 255:\n\ntrain_images[0]\n\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]], dtype=uint8)\n\n\n\npyplot.imshow(\n  train_images[0],\n\n  # color scheme\n  cmap=pyplot.cm.binary\n)\n\npyplot.show()\n\n\n\n\nEach label is a number between 0 and 9:\n\ntrain_labels[0]\n\n5"
  },
  {
    "objectID": "mnist.html#normalization",
    "href": "mnist.html#normalization",
    "title": "3  MNIST",
    "section": "3.2 Normalization",
    "text": "3.2 Normalization\nThe dataset should usually be normalized before fed into a neural network.\nFirst, the images should be converted to vectors:\n\ntrain_images = train_images.reshape((60000, 28 * 28))\n\ntrain_images[0]\n\narray([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0], dtype=uint8)\n\n\nSecond, the numbers in the images are normalized to from 0 to 1:\n\ntrain_images = train_images.astype(\"float32\") / 255\n\ntrain_images[0]\n\narray([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.05490196,\n       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.54509807,\n       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.15294118,\n       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        ], dtype=float32)\n\n\nThird, the labels should use one-hot representation:\n\n_ = np.zeros((train_labels.size, 10))\n\nfor i, label in enumerate(train_labels):\n  _[i][label] = 1\n\ntrain_labels = _\n\ntrain_labels[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
  },
  {
    "objectID": "numerical-differentiation.html",
    "href": "numerical-differentiation.html",
    "title": "4  Numerical Differentiation",
    "section": "",
    "text": "Let’s build and train a neural network to classify MNIST dataset. Numerical differentiation rather than backpropagation will be used for calculating gradient."
  },
  {
    "objectID": "numerical-differentiation.html#basic-idea",
    "href": "numerical-differentiation.html#basic-idea",
    "title": "4  Numerical Differentiation",
    "section": "4.1 Basic Idea",
    "text": "4.1 Basic Idea\nThe loss function of a neural network is a function of its parameters:\n\\[L = f(w_0, w_1, \\dots, b_0, b_1, \\dots)\\]\nThe parameters are weights and biases.\nTraining a neural network is the process of moving these parameters against their partial derivatives to gradually decrease its loss.\nThe partial derivative of some weight:\n\\[\\displaystyle \\frac{\\mathrm{d}L}{\\mathrm{d}w_i} =\n\\frac{f(\\dots, w_i + h, \\dots) - f(\\dots, w_i, \\dots)}{h}\\]\nMoving the weight against its partial derivative:\n\\[\\displaystyle w_i \\gets w_i -\n\\frac{\\mathrm{d}L}{\\mathrm{d}w_i} \\eta\\]\n\\(\\eta\\) is called learning rate."
  },
  {
    "objectID": "numerical-differentiation.html#implementation",
    "href": "numerical-differentiation.html#implementation",
    "title": "4  Numerical Differentiation",
    "section": "4.2 Implementation",
    "text": "4.2 Implementation\nThe code is in code/numerical_differentiation.py.\nThe losses of 300 iterations:"
  },
  {
    "objectID": "numpy.html",
    "href": "numpy.html",
    "title": "5  NumPy Basics",
    "section": "",
    "text": "import numpy as np\n\n# set seed\nnp.random.seed(42)\nLet’s take a break and learn some NumPy basics."
  },
  {
    "objectID": "numpy.html#create-array",
    "href": "numpy.html#create-array",
    "title": "5  NumPy Basics",
    "section": "5.1 Create Array",
    "text": "5.1 Create Array\nLet’s start with the functions we have been using.\nTurn a list into an array:\n\nnp.array([[1, 2], [3, 4]])\n\narray([[1, 2],\n       [3, 4]])\n\n\nGenerate a sequence of numbers:\n\nnp.arange(1, 10, 2)\n\narray([1, 3, 5, 7, 9])\n\n\nCreate an array of random numbers from the standard normal distribution:\n\nnp.random.randn(2, 3, 4)\n\narray([[[ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986],\n        [-0.23415337, -0.23413696,  1.57921282,  0.76743473],\n        [-0.46947439,  0.54256004, -0.46341769, -0.46572975]],\n\n       [[ 0.24196227, -1.91328024, -1.72491783, -0.56228753],\n        [-1.01283112,  0.31424733, -0.90802408, -1.4123037 ],\n        [ 1.46564877, -0.2257763 ,  0.0675282 , -1.42474819]]])\n\n\nCreate an array of zeros:\n\nnp.zeros(3)\n\narray([0., 0., 0.])\n\n\n\nnp.zeros_like(np.random.randn(2, 3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])"
  },
  {
    "objectID": "numpy.html#data-types",
    "href": "numpy.html#data-types",
    "title": "5  NumPy Basics",
    "section": "5.2 Data Types",
    "text": "5.2 Data Types\nThe data type of NumPy arrays in Python:\n\ntype(np.random.randn(2, 3))\n\nnumpy.ndarray\n\n\nndarray is short for n-dimensional array.\nNumPy data types:\n\nnp.random.randn(2, 3).dtype\n\ndtype('float64')\n\n\n\nnp.array([1, 2]).dtype\n\ndtype('int64')"
  },
  {
    "objectID": "numpy.html#array-structure",
    "href": "numpy.html#array-structure",
    "title": "5  NumPy Basics",
    "section": "5.3 Array Structure",
    "text": "5.3 Array Structure\n\na = np.random.randn(2, 3, 4)\n\n\na.ndim\n\n3\n\n\n\na.shape\n\n(2, 3, 4)"
  },
  {
    "objectID": "numpy.html#arithmetic-operations",
    "href": "numpy.html#arithmetic-operations",
    "title": "5  NumPy Basics",
    "section": "5.4 Arithmetic Operations",
    "text": "5.4 Arithmetic Operations\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[-1, -2], [-3, -4]])\nx = np.array([1, 2])\n\nElement-wise operations:\n\nA + B\n\narray([[0, 0],\n       [0, 0]])\n\n\n\nA * B\n\narray([[ -1,  -4],\n       [ -9, -16]])\n\n\nBroadcast:\n\nA * x\n\narray([[1, 4],\n       [3, 8]])"
  },
  {
    "objectID": "numpy.html#matrix-multiplication",
    "href": "numpy.html#matrix-multiplication",
    "title": "5  NumPy Basics",
    "section": "5.5 Matrix Multiplication",
    "text": "5.5 Matrix Multiplication\n\nx = np.array([1, 2])\nW = np.array([[1, 2, 3], [2, 3, 4]])\nnp.dot(x, W)\n\narray([ 5,  8, 11])\n\n\nFor batch:\n\nxs = np.array([[1, 2], [-1, -2]])\nnp.dot(xs, W)\n\narray([[  5,   8,  11],\n       [ -5,  -8, -11]])\n\n\nNote that the input to a neural network is usually a batch. The implementation of the batch version of some functions may be different. For example, below is the function calculating the sum of a vector:\n\nnp.sum(x)\n\n3\n\n\nYou can not just apply it to a batch of vectors:\n\nnp.sum(xs)\n\n0\n\n\nInstead, you should use:\n\nnp.sum(xs, axis=1, keepdims=True)\n\narray([[ 3],\n       [-3]])"
  },
  {
    "objectID": "backpropagation.html",
    "href": "backpropagation.html",
    "title": "6  Backpropagation",
    "section": "",
    "text": "Backpropagation is a more efficient algorithm for calculating gradient."
  },
  {
    "objectID": "backpropagation.html#chain-rule",
    "href": "backpropagation.html#chain-rule",
    "title": "6  Backpropagation",
    "section": "6.1 Chain Rule",
    "text": "6.1 Chain Rule\nLet’s first recall the chain rule for differentiation.\nSuppose we have\n\\[x + y = t\\]\n\\[t^2 = z\\]\nThe chain rule states that\n\\[\\displaystyle \\frac{\\partial z}{\\partial x} =\n\\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} =\n2t \\times 1 = 2(x + y)\\]"
  },
  {
    "objectID": "backpropagation.html#backpropagation",
    "href": "backpropagation.html#backpropagation",
    "title": "6  Backpropagation",
    "section": "6.2 Backpropagation",
    "text": "6.2 Backpropagation\nRecall that a neural network has many parameters. The training of a neural network is moving its parameters against their partial derivatives to decrease its loss.\nSuppose there is a weight \\(w_i\\), we can use numerical differentiation to calculate its partial derivative:\n\\[\\displaystyle \\frac{\\partial L}{\\partial w_i} =\n\\frac{f(\\dots, w_i + h, \\dots) - f(\\dots, w_i, \\dots)}{h}\\]\nNow with chain rule, we can calculate the derivative like this:\n\\[\\frac{\\partial L}{\\partial w_i} =\n\\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial b}\n\\dots \\frac{\\partial z}{\\partial w_i}\\]\n\\(a, b, \\dots, z\\) are some intermediate values."
  }
]