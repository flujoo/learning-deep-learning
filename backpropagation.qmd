# Backpropagation

Backpropagation is a more efficient algorithm for calculating gradient.


## Chain Rule

Let's first recall the chain rule for differentiation.

Suppose we have

$$x + y = t$$

$$t^2 = z$$

The chain rule states that

$$\displaystyle \frac{\partial z}{\partial x} =
\frac{\partial z}{\partial t} \frac{\partial t}{\partial x} =
2t \times 1 = 2(x + y)$$


## Backpropagation

Recall that a neural network has many parameters. The training of a neural network is moving its parameters against their partial derivatives to decrease its loss.

Suppose there is a weight $w_i$, we can use numerical differentiation to calculate its partial derivative:

$$\displaystyle \frac{\partial L}{\partial w_i} =
\frac{f(\dots, w_i + h, \dots) - f(\dots, w_i, \dots)}{h}$$

Now with chain rule, we can calculate the derivative like this:

$$\frac{\partial L}{\partial w_i} =
\frac{\partial L}{\partial a} \frac{\partial a}{\partial b}
\dots \frac{\partial z}{\partial w_i}$$

$a, b, \dots, z$ are some intermediate values.
