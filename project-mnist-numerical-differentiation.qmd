# Project: MNIST (Numerical Differentiation)

```{python}
import numpy as np
```

Let's build and train a neural network to classify MNIST dataset. In this chapter, numerical differentiation rather than backpropagation will be used to calculate gradient.


## Neural Network Methods

The function for initializing a neural network:

```{python}
def initialize(size, std=0.01):
  Ws = []
  bs = []

  for i, n in enumerate(size[:-1]):
    m = size[i+1]
    W = np.random.randn(n, m) * std
    b = np.zeros(m)
    Ws.append(W)
    bs.append(b)

  nn = {'Weights': Ws, 'biases': bs}
  return nn
```

The function for calculating the output:

```{python}
def predict(nn, x):
  Ws = nn['Weights']
  bs = nn['biases']

  for i, W in enumerate(Ws):
    b = bs[i]
    x = np.dot(x, W) + b

    if (i != len(Ws) - 1):
      x = sigmoid(x)
    else:
      x = softmax(x)

  return x
```

Sigmoid function and softmax function:

```{python}
def sigmoid(x):
  y = 1 / (1 + np.exp(-x))
  return y

def softmax(x):
  e = np.exp(x)
  y = e / np.sum(e)
  return y
```
